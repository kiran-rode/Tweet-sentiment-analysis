# Tweet-sentiment-analysis

![image](https://user-images.githubusercontent.com/82984106/117086400-08960c80-ad12-11eb-9719-2aef16375ea4.png)

BERT: Bidirectional Encoder Representations from Transformers. This technology enables anyone to train their own state-of-the-art question answering system. I wanted to try my hands on this architecture to leverage its power for sentiment analysis on the US Airlines Tweets dataset!

One of the most important aspect of BERT and why is it considered as the breakthrough in the NLP domain is because it takes word context into consideration!

BERT is a huge model, with 24 Transformer blocks, 1024 hidden units in each layer, and 340M parameters. The model is pre-trained on 40 epochs over a 3.3 billion word corpus, including BooksCorpus (800 million words) and English Wikipedia (2.5 billion words). The model runs on 16 TPU pods for training.
